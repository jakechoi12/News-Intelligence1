"""
AI Analyzer using Google Gemini

Analyzes news articles for:
- Category classification (Crisis, Ocean, Air, Inland, Economy, ETC)
- Sentiment analysis (positive, negative, neutral)
- Country/region extraction
- Keyword extraction
"""

import os
import json
import logging
import time
from typing import List, Dict, Any, Optional
from datetime import datetime

logger = logging.getLogger("analyzer.Gemini")

# Category definitions
CATEGORIES = {
    'Crisis': 'íŒŒì—…, ì‚¬ê³ , ë¶„ìŸ, ì¬í•´ ë“± ìœ„ê¸° ìƒí™©',
    'Ocean': 'í•´ìš´, ì»¨í…Œì´ë„ˆ, í•­ë§Œ, ì„ ë°• ê´€ë ¨',
    'Air': 'í•­ê³µ í™”ë¬¼, ê³µí•­, í•­ê³µì‚¬ ê´€ë ¨',
    'Inland': 'ë‚´ë¥™ ìš´ì†¡, íŠ¸ëŸ­, ì² ë„, ì°½ê³  ê´€ë ¨',
    'Economy': 'ê²½ì œ, ìš´ì„, ìˆ˜ìš”, ë¬´ì—­, ê¸ˆìœµ ê´€ë ¨',
    'ETC': 'ê¸°íƒ€ ë¬¼ë¥˜/ê³µê¸‰ë§ ë‰´ìŠ¤',
}

# Crisis keywords for quick classification
CRISIS_KEYWORDS = [
    'strike', 'crisis', 'disruption', 'closure', 'disaster', 'attack',
    'war', 'conflict', 'shortage', 'congestion', 'delay', 'accident',
    'íŒŒì—…', 'ìœ„ê¸°', 'í˜¼ì¡', 'ì‚¬ê³ ', 'ì§€ì—°', 'íì‡„', 'ë¶„ìŸ', 'ê³µê²©', 'ì¬í•´',
]

# Negative sentiment keywords
NEGATIVE_KEYWORDS = [
    'decline', 'drop', 'fall', 'crash', 'loss', 'concern', 'risk', 'threat',
    'warning', 'trouble', 'problem', 'failure', 'worst', 'critical',
    'í•˜ë½', 'ê°ì†Œ', 'ìœ„í—˜', 'ìš°ë ¤', 'ì†ì‹¤', 'ë¬¸ì œ', 'ì•…í™”', 'ìµœì•…', 'ìœ„ê¸°',
]


class GeminiAnalyzer:
    """
    Analyzes news articles using Google Gemini AI.
    Falls back to rule-based analysis if API is unavailable.
    """
    
    def __init__(self, api_key: str = None):
        """
        Initialize Gemini analyzer.
        
        Args:
            api_key: Gemini API key (uses env var if not provided)
        """
        self.api_key = api_key or os.getenv('GEMINI_API_KEY')
        self.model = None
        self._init_gemini()
        
        self.stats = {
            'total_analyzed': 0,
            'ai_analyzed': 0,
            'rule_analyzed': 0,
            'errors': 0,
        }
    
    def _init_gemini(self):
        """Initialize Gemini model"""
        if not self.api_key:
            logger.warning("âš ï¸ GEMINI_API_KEY not set. Using rule-based analysis only.")
            return
        
        try:
            import google.generativeai as genai
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel('gemini-2.0-flash')
            logger.info("âœ… Gemini model initialized successfully")
        except ImportError:
            logger.warning("âš ï¸ google-generativeai not installed. Using rule-based analysis.")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to initialize Gemini: {e}")
    
    def analyze_articles(self, articles: List[Dict[str, Any]], batch_size: int = 20) -> List[Dict[str, Any]]:
        """
        Analyze multiple articles with parallel processing.
        
        Args:
            articles: List of article dictionaries
            batch_size: Number of articles to process in parallel
            
        Returns:
            List of analyzed article dictionaries
        """
        logger.info(f"{'='*60}")
        logger.info(f"ğŸ¤– Starting AI Analysis (Parallel Processing)")
        logger.info(f"   Total articles: {len(articles)}")
        logger.info(f"{'='*60}")
        
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        def analyze_batch(batch_articles, start_idx):
            """Analyze a batch of articles"""
            batch_results = []
            for idx, article in enumerate(batch_articles):
                try:
                    analyzed_article = self._analyze_single(article)
                    batch_results.append((start_idx + idx, analyzed_article))
                except Exception as e:
                    logger.debug(f"Analysis error for article {start_idx + idx}: {e}")
                    # Keep original article with default values
                    article['category'] = 'ETC'
                    article['sentiment'] = 'neutral'
                    article['is_crisis'] = False
                    article['country_tags'] = []
                    article['keywords'] = []
                    batch_results.append((start_idx + idx, article))
            return batch_results
        
        # Process articles in parallel batches
        analyzed = [None] * len(articles)
        total_processed = 0
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = []
            for i in range(0, len(articles), batch_size):
                batch = articles[i:i+batch_size]
                future = executor.submit(analyze_batch, batch, i)
                futures.append(future)
            
            for future in as_completed(futures):
                batch_results = future.result()
                for idx, result in batch_results:
                    analyzed[idx] = result
                    self.stats['total_analyzed'] += 1
                    total_processed += 1
                    
                    if total_processed % 50 == 0:
                        logger.info(f"   Analyzing... {total_processed}/{len(articles)}")
        
        # Filter out None values (shouldn't happen, but safety check)
        analyzed = [a for a in analyzed if a is not None]
        
        logger.info(f"{'='*60}")
        logger.info(f"âœ… Analysis complete")
        logger.info(f"   Total: {self.stats['total_analyzed']}")
        logger.info(f"   AI analyzed: {self.stats['ai_analyzed']}")
        logger.info(f"   Rule-based: {self.stats['rule_analyzed']}")
        logger.info(f"   Errors: {self.stats['errors']}")
        logger.info(f"{'='*60}")
        
        return analyzed
    
    def _analyze_single(self, article: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze a single article"""
        title = article.get('title', '')
        summary = article.get('content_summary', '')
        text = f"{title} {summary}".lower()
        
        # Try AI analysis first, fall back to rules
        if self.model:
            try:
                result = self._analyze_with_ai(article)
                if result:
                    self.stats['ai_analyzed'] += 1
                    return result
            except Exception as e:
                logger.debug(f"AI analysis failed, using rules: {e}")
        
        # Rule-based analysis
        self.stats['rule_analyzed'] += 1
        return self._analyze_with_rules(article, text)
    
    def _analyze_with_ai(self, article: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Analyze article using Gemini AI"""
        title = article.get('title', '')
        summary = article.get('content_summary', '')
        
        prompt = f"""Analyze this logistics/supply chain news article and provide a JSON response:

Title: {title}
Summary: {summary}

Respond with ONLY a JSON object (no markdown, no explanation):
{{
    "category": "one of: Crisis, Ocean, Air, Inland, Economy, ETC",
    "sentiment": "one of: positive, negative, neutral",
    "is_crisis": true or false,
    "country_tags": ["ISO country codes mentioned, e.g., US, KR, CN"],
    "keywords": ["3-5 key terms from the article"]
}}

Categories:
- Crisis: Strikes, accidents, conflicts, disasters (actual ongoing incidents)
- Ocean: Maritime shipping, containers, ports, shipbuilding, marine research, KRISO
- Air: Air cargo, airports, airlines
- Inland: Trucking, rail, warehousing
- Economy: Economic indicators, freight rates, trade
- ETC: Other logistics news

IMPORTANT: Technology development, R&D success, system innovation news should NOT be classified as Crisis.
For example, "AI-based damage control system development success" is Ocean, not Crisis."""

        try:
            response = self.model.generate_content(prompt)
            text = response.text.strip()
            
            # Clean up response
            if text.startswith('```'):
                text = text.split('\n', 1)[1]
                text = text.rsplit('```', 1)[0]
            
            result = json.loads(text)
            
            # Merge with original article
            article['category'] = result.get('category', 'ETC')
            article['sentiment'] = result.get('sentiment', 'neutral')
            article['is_crisis'] = result.get('is_crisis', False)
            article['country_tags'] = result.get('country_tags', [])
            article['keywords'] = result.get('keywords', [])
            
            # Rate limiting for Gemini API
            time.sleep(0.1)
            
            return article
            
        except json.JSONDecodeError:
            logger.debug("Failed to parse AI response as JSON")
            return None
        except Exception as e:
            logger.debug(f"AI analysis error: {e}")
            return None
    
    def _analyze_with_rules(self, article: Dict[str, Any], text: str) -> Dict[str, Any]:
        """Analyze article using rule-based approach"""
        
        # Category classification
        category = self._classify_category(text)
        article['category'] = category
        
        # Sentiment analysis
        sentiment = self._classify_sentiment(text)
        article['sentiment'] = sentiment
        
        # Crisis detection
        is_crisis = category == 'Crisis' or any(kw in text for kw in CRISIS_KEYWORDS)
        article['is_crisis'] = is_crisis
        
        # Country extraction (simple)
        article['country_tags'] = self._extract_countries(text)
        
        # Keyword extraction (simple)
        article['keywords'] = self._extract_keywords(text)
        
        return article
    
    def _classify_category(self, text: str) -> str:
        """Rule-based category classification"""
        text_lower = text.lower()
        
        # Technology/Development keywords (not Crisis)
        tech_positive_keywords = ['êµ­ì‚°í™”', 'ì„±ê³µ', 'ê°œë°œ', 'ê¸°ìˆ ', 'ì‹œìŠ¤í…œ', 'development', 
                                  'technology', 'innovation', 'research', 'ì—°êµ¬', 'í˜ì‹ ']
        has_tech_positive = any(kw in text_lower for kw in tech_positive_keywords)
        
        # Ocean/Maritime (check before Crisis to prioritize domain)
        ocean_keywords = ['ship', 'port', 'container', 'maritime', 'vessel', 'cargo ship',
                         'ì„ ë°•', 'í•­ë§Œ', 'ì»¨í…Œì´ë„ˆ', 'í•´ìš´', 'ì„ ì‚¬', 'kriso', 'í•´ì–‘', 
                         'ì†ìƒí†µì œ', 'ì¡°ì„ ', 'í•´ì‚¬', 'í•´ìˆ˜ë¶€']
        if any(kw in text_lower for kw in ocean_keywords):
            # If it's a tech/development news in ocean domain, it's Ocean, not Crisis
            if has_tech_positive:
                return 'Ocean'
            # Check if it's actually a crisis in ocean domain
            if any(kw in text_lower for kw in CRISIS_KEYWORDS):
                return 'Crisis'
            return 'Ocean'
        
        # Crisis indicators (only if not tech/development news)
        if not has_tech_positive and any(kw in text_lower for kw in CRISIS_KEYWORDS):
            return 'Crisis'
        
        # Air
        air_keywords = ['air cargo', 'airport', 'airline', 'flight', 'aviation',
                       'í•­ê³µ', 'ê³µí•­', 'í™”ë¬¼ê¸°']
        if any(kw in text_lower for kw in air_keywords):
            return 'Air'
        
        # Inland
        inland_keywords = ['truck', 'rail', 'warehouse', 'distribution', 'last mile',
                          'íŠ¸ëŸ­', 'ì² ë„', 'ì°½ê³ ', 'ë¬¼ë¥˜ì„¼í„°', 'ë°°ì†¡']
        if any(kw in text_lower for kw in inland_keywords):
            return 'Inland'
        
        # Economy
        economy_keywords = ['rate', 'price', 'cost', 'trade', 'economy', 'tariff', 'gdp',
                           'ìš´ì„', 'ìš”ê¸ˆ', 'ë¬´ì—­', 'ê²½ì œ', 'ê´€ì„¸']
        if any(kw in text_lower for kw in economy_keywords):
            return 'Economy'
        
        return 'ETC'
    
    def _classify_sentiment(self, text: str) -> str:
        """Rule-based sentiment classification"""
        text_lower = text.lower()
        
        negative_count = sum(1 for kw in NEGATIVE_KEYWORDS if kw in text_lower)
        
        positive_keywords = [
            'growth', 'increase', 'rise', 'recovery', 'improve', 'success', 'award',
            'achievement', 'record', 'best', 'leading', 'innovation', 'partnership',
            'ì„±ì¥', 'ì¦ê°€', 'ìƒìŠ¹', 'íšŒë³µ', 'ê°œì„ ', 'í˜¸ì¡°', 'ìš°ìˆ˜', 'ì¸ì¦', 'ìˆ˜ìƒ',
            'ìƒìƒ', 'í˜‘ë ¥', 'ë‹¬ì„±', 'ì„±ê³µ', 'ìµœê³ ', 'ì„ ì •', 'í˜ì‹ ', 'ë„ì…', 'ì²´ê²°'
        ]
        positive_count = sum(1 for kw in positive_keywords if kw in text_lower)
        
        if negative_count > positive_count:
            return 'negative'
        elif positive_count > negative_count:
            return 'positive'
        return 'neutral'
    
    def _extract_countries(self, text: str) -> List[str]:
        """Extract country codes from text"""
        text_upper = text.upper()
        
        country_mapping = {
            'UNITED STATES': 'US', 'USA': 'US', 'AMERICA': 'US', 'ë¯¸êµ­': 'US',
            'CHINA': 'CN', 'CHINESE': 'CN', 'ì¤‘êµ­': 'CN',
            'KOREA': 'KR', 'KOREAN': 'KR', 'í•œêµ­': 'KR',
            'JAPAN': 'JP', 'JAPANESE': 'JP', 'ì¼ë³¸': 'JP',
            'GERMANY': 'DE', 'GERMAN': 'DE', 'ë…ì¼': 'DE',
            'SINGAPORE': 'SG', 'ì‹±ê°€í¬ë¥´': 'SG',
            'TAIWAN': 'TW', 'ëŒ€ë§Œ': 'TW',
            'VIETNAM': 'VN', 'ë² íŠ¸ë‚¨': 'VN',
            'INDIA': 'IN', 'ì¸ë„': 'IN',
            'NETHERLANDS': 'NL', 'DUTCH': 'NL', 'ë„¤ëœë€ë“œ': 'NL',
            'UK': 'GB', 'BRITAIN': 'GB', 'BRITISH': 'GB', 'ì˜êµ­': 'GB',
            'FRANCE': 'FR', 'FRENCH': 'FR', 'í”„ë‘ìŠ¤': 'FR',
            'RUSSIA': 'RU', 'RUSSIAN': 'RU', 'ëŸ¬ì‹œì•„': 'RU',
            'UKRAINE': 'UA', 'ìš°í¬ë¼ì´ë‚˜': 'UA',
            'IRAN': 'IR', 'ì´ë€': 'IR',
            'SAUDI': 'SA', 'ì‚¬ìš°ë””': 'SA',
            'UAE': 'AE', 'ì•„ëì—ë¯¸ë¦¬íŠ¸': 'AE',
            'YEMEN': 'YE', 'ì˜ˆë©˜': 'YE',
        }
        
        found = set()
        for keyword, code in country_mapping.items():
            if keyword in text_upper:
                found.add(code)
        
        return list(found)[:5]  # Limit to 5 countries
    
    def _extract_keywords(self, text: str) -> List[str]:
        """Extract keywords from text (simple approach)"""
        # Common logistics keywords to look for
        keywords_to_check = [
            'strike', 'port', 'shipping', 'freight', 'container', 'delay',
            'disruption', 'supply chain', 'logistics', 'cargo', 'tariff',
            'trade', 'export', 'import', 'crisis', 'congestion',
            'íŒŒì—…', 'í•­ë§Œ', 'í•´ìš´', 'ë¬¼ë¥˜', 'ì»¨í…Œì´ë„ˆ', 'ì§€ì—°', 'ìœ„ê¸°',
        ]
        
        text_lower = text.lower()
        found = [kw for kw in keywords_to_check if kw in text_lower]
        
        return found[:10]  # Limit to 10 keywords
    
    def generate_insights(self, article: Dict[str, Any]) -> Dict[str, str]:
        """
        Generate trade/logistics/SCM insights for a headline article using LLM.
        
        Args:
            article: Article dictionary with title and content_summary
            
        Returns:
            Dictionary with 'trade', 'logistics', 'scm' insights (empty if LLM fails)
        """
        title = article.get('title', '')
        summary = article.get('content_summary', '')
        
        if self.model:
            try:
                return self._generate_insights_with_ai(title, summary)
            except Exception as e:
                logger.debug(f"AI insights generation failed: {e}")
        
        # LLM ì‹¤íŒ¨ ì‹œ ë¹ˆ ì‹œì‚¬ì  ë°˜í™˜ (UIì—ì„œ "ì‹œì‚¬ì  ì—†ìŒ" í‘œì‹œ)
        return {'trade': '', 'logistics': '', 'scm': ''}
    
    def _generate_insights_with_ai(self, title: str, summary: str) -> Dict[str, str]:
        """Generate insights using Gemini AI - comprehensive summary"""
        prompt = f"""ë‹¹ì‹ ì€ ë¬´ì—­, ë¬¼ë¥˜, SCM ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì½ê³  ì¢…í•©ì ì¸ ì‹œì‚¬ì ì„ 3ì¤„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ğŸ“° ê¸°ì‚¬ ì œëª©: {title}
ğŸ“ ê¸°ì‚¬ ìš”ì•½: {summary}

ìš”ì²­ì‚¬í•­:
- ë¬´ì—­, ë¬¼ë¥˜, SCM ê´€ì ì„ ì¢…í•©í•˜ì—¬ ì´ ê¸°ì‚¬ê°€ ì£¼ëŠ” í•µì‹¬ ì‹œì‚¬ì ì„ 3ì¤„ë¡œ ì‘ì„±
- ê° ì¤„ì€ 30~50ì ë‚´ì™¸ì˜ í•œêµ­ì–´ ë¬¸ì¥
- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ì§€ì—­, ê¸°ì—…ëª…, ì˜í–¥ ë²”ìœ„ë¥¼ í¬í•¨
- ì¼ë°˜ì ì¸ ì¡°ì–¸ì´ ì•„ë‹Œ ì´ ê¸°ì‚¬ì— íŠ¹í™”ëœ ë‚´ìš©
- í‹€ì— ë§ì¶”ì§€ ë§ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì¢…í•©ì ìœ¼ë¡œ ì‘ì„±

ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ (ë§ˆí¬ë‹¤ìš´, ì„¤ëª… ì—†ì´):
{{
    "insight1": "ì²« ë²ˆì§¸ ì‹œì‚¬ì  (ë¬´ì—­/ë¬¼ë¥˜/SCM ì¢…í•©)",
    "insight2": "ë‘ ë²ˆì§¸ ì‹œì‚¬ì  (ë¬´ì—­/ë¬¼ë¥˜/SCM ì¢…í•©)",
    "insight3": "ì„¸ ë²ˆì§¸ ì‹œì‚¬ì  (ë¬´ì—­/ë¬¼ë¥˜/SCM ì¢…í•©)"
}}"""

        try:
            response = self.model.generate_content(prompt)
            text = response.text.strip()
            
            # Clean up response
            if text.startswith('```'):
                text = text.split('\n', 1)[1]
                text = text.rsplit('```', 1)[0]
            
            result = json.loads(text)
            time.sleep(0.1)  # Rate limiting
            
            # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (í•˜ìœ„ í˜¸í™˜ì„±)
            return {
                'trade': result.get('insight1', ''),
                'logistics': result.get('insight2', ''),
                'scm': result.get('insight3', ''),
            }
            
        except Exception as e:
            logger.debug(f"AI insights parsing error: {e}")
            # LLM ì‹¤íŒ¨ ì‹œ ë¹ˆ ì‹œì‚¬ì  ë°˜í™˜
            return {'trade': '', 'logistics': '', 'scm': ''}


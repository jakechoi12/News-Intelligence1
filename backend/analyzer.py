"""
AI Analyzer using Google Gemini

Analyzes news articles for:
- Category classification (Crisis, Ocean, Air, Inland, Economy, ETC)
- Sentiment analysis (positive, negative, neutral)
- Country/region extraction
- Keyword extraction
"""

import os
import json
import logging
import time
from typing import List, Dict, Any, Optional
from datetime import datetime

logger = logging.getLogger("analyzer.Gemini")

# Category definitions
CATEGORIES = {
    'Crisis': 'ÌååÏóÖ, ÏÇ¨Í≥†, Î∂ÑÏüÅ, Ïû¨Ìï¥ Îì± ÏúÑÍ∏∞ ÏÉÅÌô©',
    'Ocean': 'Ìï¥Ïö¥, Ïª®ÌÖåÏù¥ÎÑà, Ìï≠Îßå, ÏÑ†Î∞ï Í¥ÄÎ†®',
    'Air': 'Ìï≠Í≥µ ÌôîÎ¨º, Í≥µÌï≠, Ìï≠Í≥µÏÇ¨ Í¥ÄÎ†®',
    'Inland': 'ÎÇ¥Î•ô Ïö¥ÏÜ°, Ìä∏Îü≠, Ï≤†ÎèÑ, Ï∞ΩÍ≥† Í¥ÄÎ†®',
    'Economy': 'Í≤ΩÏ†ú, Ïö¥ÏûÑ, ÏàòÏöî, Î¨¥Ïó≠, Í∏àÏúµ Í¥ÄÎ†®',
    'ETC': 'Í∏∞ÌÉÄ Î¨ºÎ•ò/Í≥µÍ∏âÎßù Îâ¥Ïä§',
}

# Crisis keywords for quick classification
CRISIS_KEYWORDS = [
    'strike', 'crisis', 'disruption', 'closure', 'disaster', 'attack',
    'war', 'conflict', 'shortage', 'congestion', 'delay', 'accident',
    'ÌååÏóÖ', 'ÏúÑÍ∏∞', 'ÌòºÏû°', 'ÏÇ¨Í≥†', 'ÏßÄÏó∞', 'ÌèêÏáÑ', 'Î∂ÑÏüÅ', 'Í≥µÍ≤©', 'Ïû¨Ìï¥',
]

# Negative sentiment keywords
NEGATIVE_KEYWORDS = [
    'decline', 'drop', 'fall', 'crash', 'loss', 'concern', 'risk', 'threat',
    'warning', 'trouble', 'problem', 'failure', 'worst', 'critical',
    'ÌïòÎùΩ', 'Í∞êÏÜå', 'ÏúÑÌóò', 'Ïö∞Î†§', 'ÏÜêÏã§', 'Î¨∏Ï†ú', 'ÏïÖÌôî', 'ÏµúÏïÖ', 'ÏúÑÍ∏∞',
]


class GeminiAnalyzer:
    """
    Analyzes news articles using Google Gemini AI.
    Falls back to rule-based analysis if API is unavailable.
    """
    
    def __init__(self, api_key: str = None):
        """
        Initialize Gemini analyzer.
        
        Args:
            api_key: Gemini API key (uses env var if not provided)
        """
        self.api_key = api_key or os.getenv('GEMINI_API_KEY')
        self.model = None
        self._init_gemini()
        
        self.stats = {
            'total_analyzed': 0,
            'ai_analyzed': 0,
            'rule_analyzed': 0,
            'errors': 0,
        }
    
    def _init_gemini(self):
        """Initialize Gemini model"""
        if not self.api_key:
            logger.warning("‚ö†Ô∏è GEMINI_API_KEY not set. Using rule-based analysis only.")
            return
        
        try:
            import google.generativeai as genai
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel('gemini-2.0-flash')
            logger.info("‚úÖ Gemini model initialized successfully")
        except ImportError:
            logger.warning("‚ö†Ô∏è google-generativeai not installed. Using rule-based analysis.")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to initialize Gemini: {e}")
    
    def analyze_articles(self, articles: List[Dict[str, Any]], batch_size: int = 10) -> List[Dict[str, Any]]:
        """
        Analyze multiple articles.
        
        Args:
            articles: List of article dictionaries
            batch_size: Number of articles to analyze in each AI batch
            
        Returns:
            List of analyzed article dictionaries
        """
        logger.info(f"{'='*60}")
        logger.info(f"ü§ñ Starting AI Analysis")
        logger.info(f"   Total articles: {len(articles)}")
        logger.info(f"{'='*60}")
        
        analyzed = []
        
        for idx, article in enumerate(articles, 1):
            if idx % 50 == 0:
                logger.info(f"   Analyzing... {idx}/{len(articles)}")
            
            try:
                analyzed_article = self._analyze_single(article)
                analyzed.append(analyzed_article)
                self.stats['total_analyzed'] += 1
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è Analysis error for article {idx}: {e}")
                # Keep original article with default values
                article['category'] = 'ETC'
                article['sentiment'] = 'neutral'
                article['is_crisis'] = False
                article['country_tags'] = []
                article['keywords'] = []
                analyzed.append(article)
                self.stats['errors'] += 1
        
        logger.info(f"{'='*60}")
        logger.info(f"‚úÖ Analysis complete")
        logger.info(f"   Total: {self.stats['total_analyzed']}")
        logger.info(f"   AI analyzed: {self.stats['ai_analyzed']}")
        logger.info(f"   Rule-based: {self.stats['rule_analyzed']}")
        logger.info(f"   Errors: {self.stats['errors']}")
        logger.info(f"{'='*60}")
        
        return analyzed
    
    def _analyze_single(self, article: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze a single article"""
        title = article.get('title', '')
        summary = article.get('content_summary', '')
        text = f"{title} {summary}".lower()
        
        # Try AI analysis first, fall back to rules
        if self.model:
            try:
                result = self._analyze_with_ai(article)
                if result:
                    self.stats['ai_analyzed'] += 1
                    return result
            except Exception as e:
                logger.debug(f"AI analysis failed, using rules: {e}")
        
        # Rule-based analysis
        self.stats['rule_analyzed'] += 1
        return self._analyze_with_rules(article, text)
    
    def _analyze_with_ai(self, article: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Analyze article using Gemini AI"""
        title = article.get('title', '')
        summary = article.get('content_summary', '')
        
        prompt = f"""Analyze this logistics/supply chain news article and provide a JSON response:

Title: {title}
Summary: {summary}

Respond with ONLY a JSON object (no markdown, no explanation):
{{
    "category": "one of: Crisis, Ocean, Air, Inland, Economy, ETC",
    "sentiment": "one of: positive, negative, neutral",
    "is_crisis": true or false,
    "country_tags": ["ISO country codes mentioned, e.g., US, KR, CN"],
    "keywords": ["3-5 key terms from the article"]
}}

Categories:
- Crisis: Strikes, accidents, conflicts, disasters
- Ocean: Maritime shipping, containers, ports
- Air: Air cargo, airports, airlines
- Inland: Trucking, rail, warehousing
- Economy: Economic indicators, freight rates, trade
- ETC: Other logistics news"""

        try:
            response = self.model.generate_content(prompt)
            text = response.text.strip()
            
            # Clean up response
            if text.startswith('```'):
                text = text.split('\n', 1)[1]
                text = text.rsplit('```', 1)[0]
            
            result = json.loads(text)
            
            # Merge with original article
            article['category'] = result.get('category', 'ETC')
            article['sentiment'] = result.get('sentiment', 'neutral')
            article['is_crisis'] = result.get('is_crisis', False)
            article['country_tags'] = result.get('country_tags', [])
            article['keywords'] = result.get('keywords', [])
            
            # Rate limiting for Gemini API
            time.sleep(0.1)
            
            return article
            
        except json.JSONDecodeError:
            logger.debug("Failed to parse AI response as JSON")
            return None
        except Exception as e:
            logger.debug(f"AI analysis error: {e}")
            return None
    
    def _analyze_with_rules(self, article: Dict[str, Any], text: str) -> Dict[str, Any]:
        """Analyze article using rule-based approach"""
        
        # Category classification
        category = self._classify_category(text)
        article['category'] = category
        
        # Sentiment analysis
        sentiment = self._classify_sentiment(text)
        article['sentiment'] = sentiment
        
        # Crisis detection
        is_crisis = category == 'Crisis' or any(kw in text for kw in CRISIS_KEYWORDS)
        article['is_crisis'] = is_crisis
        
        # Country extraction (simple)
        article['country_tags'] = self._extract_countries(text)
        
        # Keyword extraction (simple)
        article['keywords'] = self._extract_keywords(text)
        
        return article
    
    def _classify_category(self, text: str) -> str:
        """Rule-based category classification"""
        text_lower = text.lower()
        
        # Crisis indicators
        if any(kw in text_lower for kw in CRISIS_KEYWORDS):
            return 'Crisis'
        
        # Ocean/Maritime
        ocean_keywords = ['ship', 'port', 'container', 'maritime', 'vessel', 'cargo ship',
                         'ÏÑ†Î∞ï', 'Ìï≠Îßå', 'Ïª®ÌÖåÏù¥ÎÑà', 'Ìï¥Ïö¥', 'ÏÑ†ÏÇ¨']
        if any(kw in text_lower for kw in ocean_keywords):
            return 'Ocean'
        
        # Air
        air_keywords = ['air cargo', 'airport', 'airline', 'flight', 'aviation',
                       'Ìï≠Í≥µ', 'Í≥µÌï≠', 'ÌôîÎ¨ºÍ∏∞']
        if any(kw in text_lower for kw in air_keywords):
            return 'Air'
        
        # Inland
        inland_keywords = ['truck', 'rail', 'warehouse', 'distribution', 'last mile',
                          'Ìä∏Îü≠', 'Ï≤†ÎèÑ', 'Ï∞ΩÍ≥†', 'Î¨ºÎ•òÏÑºÌÑ∞', 'Î∞∞ÏÜ°']
        if any(kw in text_lower for kw in inland_keywords):
            return 'Inland'
        
        # Economy
        economy_keywords = ['rate', 'price', 'cost', 'trade', 'economy', 'tariff', 'gdp',
                           'Ïö¥ÏûÑ', 'ÏöîÍ∏à', 'Î¨¥Ïó≠', 'Í≤ΩÏ†ú', 'Í¥ÄÏÑ∏']
        if any(kw in text_lower for kw in economy_keywords):
            return 'Economy'
        
        return 'ETC'
    
    def _classify_sentiment(self, text: str) -> str:
        """Rule-based sentiment classification"""
        text_lower = text.lower()
        
        negative_count = sum(1 for kw in NEGATIVE_KEYWORDS if kw in text_lower)
        
        positive_keywords = ['growth', 'increase', 'rise', 'recovery', 'improve', 'success',
                            'ÏÑ±Ïû•', 'Ï¶ùÍ∞Ä', 'ÏÉÅÏäπ', 'ÌöåÎ≥µ', 'Í∞úÏÑ†', 'Ìò∏Ï°∞']
        positive_count = sum(1 for kw in positive_keywords if kw in text_lower)
        
        if negative_count > positive_count:
            return 'negative'
        elif positive_count > negative_count:
            return 'positive'
        return 'neutral'
    
    def _extract_countries(self, text: str) -> List[str]:
        """Extract country codes from text"""
        text_upper = text.upper()
        
        country_mapping = {
            'UNITED STATES': 'US', 'USA': 'US', 'AMERICA': 'US', 'ÎØ∏Íµ≠': 'US',
            'CHINA': 'CN', 'CHINESE': 'CN', 'Ï§ëÍµ≠': 'CN',
            'KOREA': 'KR', 'KOREAN': 'KR', 'ÌïúÍµ≠': 'KR',
            'JAPAN': 'JP', 'JAPANESE': 'JP', 'ÏùºÎ≥∏': 'JP',
            'GERMANY': 'DE', 'GERMAN': 'DE', 'ÎèÖÏùº': 'DE',
            'SINGAPORE': 'SG', 'Ïã±Í∞ÄÌè¨Î•¥': 'SG',
            'TAIWAN': 'TW', 'ÎåÄÎßå': 'TW',
            'VIETNAM': 'VN', 'Î≤†Ìä∏ÎÇ®': 'VN',
            'INDIA': 'IN', 'Ïù∏ÎèÑ': 'IN',
            'NETHERLANDS': 'NL', 'DUTCH': 'NL', 'ÎÑ§ÎçúÎûÄÎìú': 'NL',
            'UK': 'GB', 'BRITAIN': 'GB', 'BRITISH': 'GB', 'ÏòÅÍµ≠': 'GB',
            'FRANCE': 'FR', 'FRENCH': 'FR', 'ÌîÑÎûëÏä§': 'FR',
            'RUSSIA': 'RU', 'RUSSIAN': 'RU', 'Îü¨ÏãúÏïÑ': 'RU',
            'UKRAINE': 'UA', 'Ïö∞ÌÅ¨ÎùºÏù¥ÎÇò': 'UA',
            'IRAN': 'IR', 'Ïù¥ÎûÄ': 'IR',
            'SAUDI': 'SA', 'ÏÇ¨Ïö∞Îîî': 'SA',
            'UAE': 'AE', 'ÏïÑÎûçÏóêÎØ∏Î¶¨Ìä∏': 'AE',
            'YEMEN': 'YE', 'ÏòàÎ©ò': 'YE',
        }
        
        found = set()
        for keyword, code in country_mapping.items():
            if keyword in text_upper:
                found.add(code)
        
        return list(found)[:5]  # Limit to 5 countries
    
    def _extract_keywords(self, text: str) -> List[str]:
        """Extract keywords from text (simple approach)"""
        # Common logistics keywords to look for
        keywords_to_check = [
            'strike', 'port', 'shipping', 'freight', 'container', 'delay',
            'disruption', 'supply chain', 'logistics', 'cargo', 'tariff',
            'trade', 'export', 'import', 'crisis', 'congestion',
            'ÌååÏóÖ', 'Ìï≠Îßå', 'Ìï¥Ïö¥', 'Î¨ºÎ•ò', 'Ïª®ÌÖåÏù¥ÎÑà', 'ÏßÄÏó∞', 'ÏúÑÍ∏∞',
        ]
        
        text_lower = text.lower()
        found = [kw for kw in keywords_to_check if kw in text_lower]
        
        return found[:5]  # Limit to 5 keywords
    
    def generate_insights(self, article: Dict[str, Any]) -> Dict[str, str]:
        """
        Generate trade/logistics/SCM insights for a headline article.
        
        Args:
            article: Article dictionary with title and content_summary
            
        Returns:
            Dictionary with 'trade', 'logistics', 'scm' insights
        """
        title = article.get('title', '')
        summary = article.get('content_summary', '')
        
        if self.model:
            try:
                return self._generate_insights_with_ai(title, summary)
            except Exception as e:
                logger.debug(f"AI insights generation failed: {e}")
        
        # Rule-based fallback
        return self._generate_insights_with_rules(title, summary)
    
    def _generate_insights_with_ai(self, title: str, summary: str) -> Dict[str, str]:
        """Generate insights using Gemini AI - specific to each article"""
        prompt = f"""ÎãπÏã†ÏùÄ Î¨¥Ïó≠, Î¨ºÎ•ò, SCM Ï†ÑÎ¨∏ Î∂ÑÏÑùÍ∞ÄÏûÖÎãàÎã§. ÏïÑÎûò Îâ¥Ïä§ Í∏∞ÏÇ¨Î•º ÏùΩÍ≥†, Ïù¥ **ÌäπÏ†ï Í∏∞ÏÇ¨**Ïóê ÎåÄÌïú Íµ¨Ï≤¥Ï†ÅÏù∏ ÏãúÏÇ¨Ï†êÏùÑ Î∂ÑÏÑùÌï¥Ï£ºÏÑ∏Ïöî.

üì∞ Í∏∞ÏÇ¨ Ï†úÎ™©: {title}
üìù Í∏∞ÏÇ¨ ÏöîÏïΩ: {summary}

Î∂ÑÏÑù ÏöîÏ≤≠:
Ïù¥ Í∏∞ÏÇ¨Í∞Ä Î¨¥Ïó≠/Î¨ºÎ•ò/SCM Îã¥ÎãπÏûêÏóêÍ≤å Ï£ºÎäî **Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† Ïã§Ìñâ Í∞ÄÎä•Ìïú** ÏãúÏÇ¨Ï†êÏùÑ ÏûëÏÑ±ÌïòÏÑ∏Ïöî.
- ÏùºÎ∞òÏ†ÅÏù∏ Ï°∞Ïñ∏Ïù¥ ÏïÑÎãå, **Ïù¥ Í∏∞ÏÇ¨Ïùò ÎÇ¥Ïö©Ïóê ÏßÅÏ†ë Ïó∞Í¥ÄÎêú** ÏãúÏÇ¨Ï†êÏù¥Ïñ¥Ïïº Ìï©ÎãàÎã§.
- Í∏∞ÏÇ¨ÏóêÏÑú Ïñ∏Í∏âÎêú **ÌäπÏ†ï ÏßÄÏó≠, Í∏∞ÏóÖ, ÌíàÎ™©, ÏàòÏπò** Îì±ÏùÑ ÌôúÏö©ÌïòÏÑ∏Ïöî.
- Í∞Å ÏãúÏÇ¨Ï†êÏùÄ 20~40Ïûê ÎÇ¥Ïô∏Ïùò ÌïúÍµ≠Ïñ¥ Ìïú Î¨∏Ïû•ÏúºÎ°ú ÏûëÏÑ±ÌïòÏÑ∏Ïöî.

ÏïÑÎûò JSON ÌòïÏãùÏúºÎ°úÎßå ÏùëÎãµ (ÎßàÌÅ¨Îã§Ïö¥, ÏÑ§Î™Ö ÏóÜÏù¥):
{{
    "trade": "Î¨¥Ïó≠ Í¥ÄÏ†ê: Ïù¥ Í∏∞ÏÇ¨Î°ú Ïù∏Ìïú ÏàòÏ∂úÏûÖ/Í¥ÄÏÑ∏/Î¨¥Ïó≠Ï†ïÏ±Ö ÏòÅÌñ•",
    "logistics": "Î¨ºÎ•ò Í¥ÄÏ†ê: Ïù¥ Í∏∞ÏÇ¨Î°ú Ïù∏Ìïú Ïö¥ÏÜ°/Î∞∞ÏÜ°/Ï∞ΩÍ≥† Ïö¥ÏòÅ ÏòÅÌñ•",
    "scm": "SCM Í¥ÄÏ†ê: Ïù¥ Í∏∞ÏÇ¨Î°ú Ïù∏Ìïú Ïû¨Í≥†/Ï°∞Îã¨/Í≥µÍ∏âÎßù Ï†ÑÎûµ ÏòÅÌñ•"
}}

ÏòàÏãú (Ï∞∏Í≥†Ïö©):
- Í∏∞ÏÇ¨: "ÌôçÌï¥ ÌõÑÌã∞ Í≥µÍ≤©ÏúºÎ°ú MSC ÏÑ†Î∞ï Ïö¥Ìï≠ Ï§ëÎã®" 
  ‚Üí trade: "ÏïÑÏãúÏïÑ-Ïú†ÎüΩ Ïö¥ÏûÑ 20% Ïù¥ÏÉÅ ÏÉÅÏäπ ÎåÄÎπÑ ÏõêÍ∞Ä Ïû¨ÏÇ∞Ï†ï ÌïÑÏöî"
  ‚Üí logistics: "ÏàòÏóêÏ¶à Ïö∞Ìöå Ïãú 14Ïùº Ï∂îÍ∞Ä ÏÜåÏöî, ÏÑ†Ï†Å ÏùºÏ†ï Ï°∞Ï†ï Í∂åÏû•"
  ‚Üí scm: "Ïú†ÎüΩÌñ• Î∂ÄÌíà ÏïàÏ†ÑÏû¨Í≥† 3Ï£º Ïù¥ÏÉÅÏúºÎ°ú ÏÉÅÌñ• Í≤ÄÌÜ†"

- Í∏∞ÏÇ¨: "Î∂ÄÏÇ∞Ìï≠ Ï≤¥ÏÑ† 2Ï£ºÏß∏ ÏßÄÏÜç"
  ‚Üí trade: "Î∂ÄÏÇ∞Ìï≠ Í≤ΩÏú† ÏàòÏ∂ú Í±¥ ÎÇ©Í∏∞ ÏßÄÏó∞ Î∂àÍ∞ÄÌîº, Í≥†Í∞ùÏÇ¨ ÏÇ¨Ï†Ñ ÌÜµÎ≥¥ ÌïÑÏöî"
  ‚Üí logistics: "Í¥ëÏñëÌï≠ ÎòêÎäî Ïù∏Ï≤úÌï≠ ÎåÄÏ≤¥ ÏÑ†Ï†Å Í≤ÄÌÜ† Í∂åÏû•"
  ‚Üí scm: "Íµ≠ÎÇ¥ Ï∂úÍ≥†Î∂Ñ ÏÑ†Ï†ú ÌôïÎ≥¥ Î∞è Ïû¨Í≥† ÏúÑÏπò Ïû¨Î∞∞Ïπò Í≥†Î†§"""

        try:
            response = self.model.generate_content(prompt)
            text = response.text.strip()
            
            # Clean up response
            if text.startswith('```'):
                text = text.split('\n', 1)[1]
                text = text.rsplit('```', 1)[0]
            
            result = json.loads(text)
            time.sleep(0.1)  # Rate limiting
            
            return {
                'trade': result.get('trade', ''),
                'logistics': result.get('logistics', ''),
                'scm': result.get('scm', ''),
            }
            
        except Exception as e:
            logger.debug(f"AI insights parsing error: {e}")
            return self._generate_insights_with_rules(title, summary)
    
    def _generate_insights_with_rules(self, title: str, summary: str) -> Dict[str, str]:
        """
        Generate insights using rule-based approach.
        Used as fallback when AI is unavailable.
        Extracts key entities from the article to provide semi-customized insights.
        """
        text = f"{title} {summary}"
        text_lower = text.lower()
        
        # Extract key entities for context-aware insights
        location = self._extract_location_from_text(text)
        company = self._extract_company_from_text(text)
        
        # Build context-aware prefix
        context = ""
        if location:
            context = f"{location} Í¥ÄÎ†® "
        elif company:
            context = f"{company} Í¥ÄÎ†® "
        
        # Determine the main topic and generate specific insights
        # Strike/Labor issues
        if any(kw in text_lower for kw in ['strike', 'labor', 'ÌååÏóÖ', 'ÎÖ∏Îèô', 'ÎÖ∏Ï°∞']):
            return {
                'trade': f"{context}ÌååÏóÖ Ïû•Í∏∞Ìôî Ïãú ÏàòÏ∂úÏûÖ ÌÜµÍ¥Ä ÏßÄÏó∞ Î∞è Ï∂îÍ∞Ä ÎπÑÏö© Î∞úÏÉù ÏòàÏÉÅ",
                'logistics': f"{context}ÎåÄÏ≤¥ Ìï≠Îßå/ÌÑ∞ÎØ∏ÎÑê ÌôïÎ≥¥ Î∞è Í∏¥Í∏â Ïö¥ÏÜ° Î£®Ìä∏ Í≤ÄÌÜ† ÌïÑÏöî",
                'scm': f"{context}ÌååÏóÖ Í∏∞Í∞Ñ Í∞êÏïàÌïú ÏïàÏ†ÑÏû¨Í≥† ÌôïÎ≥¥ Î∞è ÎÇ©Í∏∞ Ïû¨Ï°∞Ï†ï Í∂åÏû•",
            }
        
        # Port/Congestion issues
        elif any(kw in text_lower for kw in ['congestion', 'ÌòºÏû°', 'Ï≤¥ÏÑ†', 'port', 'Ìï≠Îßå', 'Ìï≠Íµ¨']):
            return {
                'trade': f"{context}Ï≤¥ÏÑ† ÎπÑÏö© Î∞è ÏßÄÏó∞ ÏÜêÌï¥ Î∞úÏÉù Í∞ÄÎä•, Í≥ÑÏïΩ Ï°∞Í±¥ Ï†êÍ≤Ä ÌïÑÏöî",
                'logistics': f"{context}ÏûÖÏ∂úÌï≠ ÏùºÏ†ï Ïû¨Ï°∞Ï†ï Î∞è ÎåÄÏ≤¥ Ìï≠Îßå ÌôúÏö© Í≤ÄÌÜ†",
                'scm': f"{context}Î¶¨ÎìúÌÉÄÏûÑ Ïó∞Ïû• Í∞êÏïàÌïú Î∞úÏ£º ÏãúÏ†ê ÏïûÎãπÍπÄ Í≥†Î†§",
            }
        
        # Freight rates/Cost
        elif any(kw in text_lower for kw in ['rate', 'freight', 'Ïö¥ÏûÑ', 'ÏöîÍ∏à', 'cost', 'ÎπÑÏö©']):
            return {
                'trade': f"{context}Ïö¥ÏûÑ Î≥ÄÎèôÎ∂Ñ Î∞òÏòÅÌïú ÏàòÏ∂úÏûÖ ÏõêÍ∞Ä Î∞è ÎßàÏßÑ Ïû¨Í≤ÄÌÜ† ÌïÑÏöî",
                'logistics': f"{context}Ïû•Í∏∞ Í≥ÑÏïΩ ÎòêÎäî Ïä§Ìåü Ïö¥ÏûÑ ÎπÑÍµê Î∂ÑÏÑù ÌõÑ ÏµúÏ†ÅÏïà ÏÑ†ÌÉù",
                'scm': f"{context}Î¨ºÎ•òÎπÑ ÏÉÅÏäπ ÎåÄÎπÑ Ïû¨Í≥† Ï†ïÏ±Ö Î∞è Î∞∞ÏÜ° ÎπàÎèÑ ÏµúÏ†ÅÌôî Í≤ÄÌÜ†",
            }
        
        # Geopolitical/Crisis/Attack
        elif any(kw in text_lower for kw in ['attack', 'war', 'crisis', 'Í≥µÍ≤©', 'Ï†ÑÏüÅ', 'ÏúÑÍ∏∞', 'Î∂ÑÏüÅ', 'houthi', 'ÌõÑÌã∞']):
            return {
                'trade': f"{context}Ìï¥Îãπ ÏßÄÏó≠ Í≤ΩÏú† Î¨ºÎèôÎüâ ÏòÅÌñ• Î∞è Ïö∞Ìöå ÎπÑÏö© ÏÇ∞Ï†ï ÌïÑÏöî",
                'logistics': f"{context}ÎåÄÏ≤¥ Ìï≠Î°ú(Ìù¨ÎßùÎ¥â Îì±) ÌôúÏö© Ïãú Î¶¨ÎìúÌÉÄÏûÑ Ï¶ùÍ∞Ä ÎåÄÎπÑ",
                'scm': f"{context}Î≥µÏàò ÏÜåÏã± Î∞è ÏßÄÏó≠ Î∂ÑÏÇ∞ Ï†ÑÎûµÏúºÎ°ú Î¶¨Ïä§ÌÅ¨ Ìó∑ÏßÄ Í∂åÏû•",
            }
        
        # Delay/Disruption
        elif any(kw in text_lower for kw in ['delay', 'disruption', 'ÏßÄÏó∞', 'Ï∞®Ïßà', 'Ï§ëÎã®']):
            return {
                'trade': f"{context}ÎÇ©Í∏∞ ÏßÄÏó∞Ïóê Îî∞Î•∏ Í≥†Í∞ù Ïª§ÎÆ§ÎãàÏºÄÏù¥ÏÖò Î∞è ÌéòÎÑêÌã∞ Í≤ÄÌÜ†",
                'logistics': f"{context}Í∏¥Í∏â Î∞∞ÏÜ°(Ìï≠Í≥µ Ï†ÑÌôò Îì±) ÏòµÏÖò ÎπÑÏö© ÎåÄÎπÑ Ìö®Í≥º Î∂ÑÏÑù",
                'scm': f"{context}Î≤ÑÌçº Ïû¨Í≥† ÌôïÎåÄ Î∞è ÎåÄÏ≤¥ Í≥µÍ∏âÏ≤ò ÌôúÏÑ±Ìôî Í≤ÄÌÜ†",
            }
        
        # Supply chain/Shortage
        elif any(kw in text_lower for kw in ['supply chain', 'shortage', 'Í≥µÍ∏âÎßù', 'Î∂ÄÏ°±', 'ÌíàÍ∑Ä']):
            return {
                'trade': f"{context}Í≥µÍ∏â Î∂àÏïàÏ†ïÏóê Îî∞Î•∏ ÏàòÏûÖÏÑ† Îã§Î≥ÄÌôî Í≤ÄÌÜ† ÌïÑÏöî",
                'logistics': f"{context}ÌïµÏã¨ ÌíàÎ™© Ïö∞ÏÑ† ÌôïÎ≥¥ Î∞è Î¨ºÎ•ò Ï±ÑÎÑê Îã§Í∞ÅÌôî Ï∂îÏßÑ",
                'scm': f"{context}ÏïàÏ†ÑÏû¨Í≥† ÏàòÏ§Ä ÏÉÅÌñ• Î∞è ÎåÄÏ≤¥ Î∂ÄÌíà ÏäπÏù∏ Ï†àÏ∞® Í∞ÄÏÜçÌôî",
            }
        
        # Canal/Route specific (Suez, Panama, etc.)
        elif any(kw in text_lower for kw in ['suez', 'panama', 'canal', 'ÏàòÏóêÏ¶à', 'ÌååÎÇòÎßà', 'Ïö¥Ìïò']):
            return {
                'trade': f"{context}Ïö¥Ìïò ÌÜµÍ≥º ÏßÄÏó∞/Ï†úÌïú Ïãú Ïö¥ÏÜ° ÎπÑÏö© ÏÉÅÏäπ ÎåÄÎπÑ ÌïÑÏöî",
                'logistics': f"{context}Ïö∞Ìöå Ìï≠Î°ú Ï†ÑÌôò Ïãú Ï∂îÍ∞Ä ÏÜåÏöî ÏùºÏàò Î∞è ÎπÑÏö© ÏÇ∞Ï†ï",
                'scm': f"{context}Ïû•Í∏∞Ìôî ÎåÄÎπÑ ÏÑ†Ï†úÏ†Å Ïû¨Í≥† ÌôïÎ≥¥ Î∞è ÏÉùÏÇ∞ ÏùºÏ†ï Ï°∞Ï†ï Í∂åÏû•",
            }
        
        # Default - try to be somewhat relevant
        else:
            return {
                'trade': f"{context}ÏãúÏû• ÎèôÌñ• Î≥ÄÌôîÏóê Îî∞Î•∏ ÏàòÏ∂úÏûÖ Ï†ÑÎûµ Ïû¨Í≤ÄÌÜ† ÌïÑÏöî",
                'logistics': f"{context}Ïö¥ÏòÅ Ìö®Ïú®Ìôî Î∞è ÎπÑÏö© ÏµúÏ†ÅÌôî Í∏∞Ìöå ÌÉêÏÉâ Í∂åÏû•",
                'scm': f"{context}Í≥µÍ∏âÎßù Î¶¨Ïä§ÌÅ¨ Î™®ÎãàÌÑ∞ÎßÅ Í∞ïÌôî Î∞è ÎåÄÏùë Ï≤¥Í≥Ñ Ï†êÍ≤Ä",
            }
    
    def _extract_location_from_text(self, text: str) -> str:
        """Extract primary location/port from text"""
        locations = {
            'Î∂ÄÏÇ∞': 'Î∂ÄÏÇ∞Ìï≠', 'Ïù∏Ï≤ú': 'Ïù∏Ï≤úÌï≠', 'Í¥ëÏñë': 'Í¥ëÏñëÌï≠', 'ÌèâÌÉù': 'ÌèâÌÉùÌï≠',
            'busan': 'Î∂ÄÏÇ∞Ìï≠', 'shanghai': 'ÏÉÅÌïòÏù¥Ìï≠', 'singapore': 'Ïã±Í∞ÄÌè¨Î•¥Ìï≠',
            'rotterdam': 'Î°úÌÖåÎ•¥Îã¥Ìï≠', 'los angeles': 'LAÌï≠', 'long beach': 'Î°±ÎπÑÏπòÌï≠',
            'red sea': 'ÌôçÌï¥', 'ÌôçÌï¥': 'ÌôçÌï¥', 'suez': 'ÏàòÏóêÏ¶àÏö¥Ìïò', 'ÏàòÏóêÏ¶à': 'ÏàòÏóêÏ¶àÏö¥Ìïò',
            'panama': 'ÌååÎÇòÎßàÏö¥Ìïò', 'ÌååÎÇòÎßà': 'ÌååÎÇòÎßàÏö¥Ìïò',
            'Ï§ëÍµ≠': 'Ï§ëÍµ≠', 'china': 'Ï§ëÍµ≠', 'ÎØ∏Íµ≠': 'ÎØ∏Íµ≠', 'us': 'ÎØ∏Íµ≠',
            'Ïú†ÎüΩ': 'Ïú†ÎüΩ', 'europe': 'Ïú†ÎüΩ', 'ÏùºÎ≥∏': 'ÏùºÎ≥∏', 'japan': 'ÏùºÎ≥∏',
        }
        
        text_lower = text.lower()
        for keyword, location in locations.items():
            if keyword in text_lower:
                return location
        return ""
    
    def _extract_company_from_text(self, text: str) -> str:
        """Extract primary company/carrier from text"""
        companies = {
            'maersk': 'Maersk', 'msc': 'MSC', 'cosco': 'COSCO', 'cma cgm': 'CMA CGM',
            'evergreen': 'Evergreen', 'hmm': 'HMM', 'one': 'ONE', 'hapag': 'Hapag-Lloyd',
            'Î®∏Ïä§ÌÅ¨': 'Maersk', 'ÏóêÎ≤ÑÍ∑∏Î¶∞': 'Evergreen',
            'fedex': 'FedEx', 'ups': 'UPS', 'dhl': 'DHL',
            'tesla': 'Tesla', 'apple': 'Apple', 'samsung': 'ÏÇºÏÑ±', 'ÏÇºÏÑ±': 'ÏÇºÏÑ±',
            'tsmc': 'TSMC', 'nvidia': 'NVIDIA',
        }
        
        text_lower = text.lower()
        for keyword, company in companies.items():
            if keyword in text_lower:
                return company
        return ""

